{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2902dfd9-78ae-4807-96d1-4113c5f0481e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "from typing import Deque, Dict, List, Tuple\n",
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from IPython.display import clear_output\n",
    "from torch.distributions import Normal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d962137e-2b0f-4226-b316-dffad50c97ce",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618ace2f-3db4-4155-9c07-b2762ad6c0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_layer_uniform(layer: nn.Linear, init_w: float = 3e-3) -> nn.Linear:\n",
    "    \"\"\"Init uniform parameters on the single layer.\"\"\"\n",
    "    layer.weight.data.uniform_(-init_w, init_w)\n",
    "    layer.bias.data.uniform_(-init_w, init_w)\n",
    "\n",
    "    return layer\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_dim: int, \n",
    "        out_dim: int, \n",
    "        log_std_min: int = -20,\n",
    "        log_std_max: int = 0,\n",
    "    ):\n",
    "        \"\"\"Initialize.\"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.log_std_min = log_std_min\n",
    "        self.log_std_max = log_std_max\n",
    "        self.hidden = nn.Linear(in_dim, 32)\n",
    "\n",
    "        self.mu_layer = nn.Linear(32, out_dim)\n",
    "        self.mu_layer = init_layer_uniform(self.mu_layer)\n",
    "\n",
    "        self.log_std_layer = nn.Linear(32, out_dim)\n",
    "        self.log_std_layer = init_layer_uniform(self.log_std_layer)\n",
    "\n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward method implementation.\"\"\"\n",
    "        x = F.relu(self.hidden(state))\n",
    "        \n",
    "        mu = torch.tanh(self.mu_layer(x))\n",
    "        log_std = torch.tanh(self.log_std_layer(x))\n",
    "        log_std = self.log_std_min + 0.5 * (\n",
    "            self.log_std_max - self.log_std_min\n",
    "        ) * (log_std + 1)\n",
    "        std = torch.exp(log_std)\n",
    "\n",
    "        dist = Normal(mu, std)\n",
    "        action = dist.sample()\n",
    "\n",
    "        return action, dist\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, in_dim: int):\n",
    "        \"\"\"Initialize.\"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.hidden = nn.Linear(in_dim, 64)\n",
    "        self.out = nn.Linear(64, 1)\n",
    "        self.out = init_layer_uniform(self.out)\n",
    "\n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward method implementation.\"\"\"\n",
    "        x = F.relu(self.hidden(state))\n",
    "        value = self.out(x)\n",
    "\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc126ab-8e5f-46b2-853b-371009284884",
   "metadata": {},
   "source": [
    "# GAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501d3ac8-636b-476d-941d-19edacbdaeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(\n",
    "    next_value: list, \n",
    "    rewards: list, \n",
    "    masks: list, \n",
    "    values: list, \n",
    "    gamma: float, \n",
    "    tau: float\n",
    ") -> List:\n",
    "    \"\"\"Compute gae.\"\"\"\n",
    "    values = values + [next_value]\n",
    "    gae = 0\n",
    "    returns: Deque[float] = deque()\n",
    "\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = (\n",
    "            rewards[step]\n",
    "            + gamma * values[step + 1] * masks[step]\n",
    "            - values[step]\n",
    "        )\n",
    "        gae = delta + gamma * tau * masks[step] * gae\n",
    "        returns.appendleft(gae + values[step])\n",
    "\n",
    "    return list(returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b9e981-f43c-46bc-8f82-936e6449353a",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dcab63-8957-4e7e-a9a4-699b655c3604",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_state():\n",
    "    \"\"\"\n",
    "    Retrieve the state of the intersection from sumo, in the form of cell occupancy\n",
    "    \"\"\"\n",
    "    state = np.zeros((3, 208, 206))   ## kind of like an RGB image\n",
    "    lane = [\"N2TL_0\",\"N2TL_1\",\"N2TL_2\",\"E2TL_0\",\"E2TL_1\",\"E2TL_2\",\"E2TL_3\",\"S2TL_0\",\"S2TL_1\",\"S2TL_2\",\"W2TL_0\",\"W2TL_1\",\"W2TL_2\",\"W2TL_3\"]\n",
    "    # N, E, S, W\n",
    "    #           N\n",
    "    #   W               E\n",
    "    #           S    \n",
    "    car_list = traci.vehicle.getIDList()\n",
    "\n",
    "    for car_id in car_list:\n",
    "        lane_pos = traci.vehicle.getLanePosition(car_id)\n",
    "        car_speed = traci.vehicle.getSpeed(car_id)\n",
    "        lane_id = traci.vehicle.getLaneID(car_id)\n",
    "\n",
    "        if 'N2TL' in lane_id:            \n",
    "            x = 99 + int(lane_id[-1])\n",
    "            y = int(lane_pos//7.5)\n",
    "            state[0][y][x] = 1 #presence / volume\n",
    "            state[1][y][x] = car_speed #velocity\n",
    "            state[2][y][x] = traci.vehicle.getAccumulatedWaitingTime(car_id) #waitingTime\n",
    "            \n",
    "        if 'E2TL' in lane_id:\n",
    "            x = 205 - int(lane_pos//7.5)\n",
    "            y = 99 + int(lane_id[-1])\n",
    "            state[0][y][x] = 1 #presence / volume\n",
    "            state[1][y][x] = car_speed #velocity\n",
    "            state[2][y][x] = traci.vehicle.getAccumulatedWaitingTime(car_id) #waitingTime\n",
    "\n",
    "        if 'S2TL' in lane_id:\n",
    "            x = 99 + 2 + int(lane_id[-1])\n",
    "            y = 207 - int(lane_pos//7.5)\n",
    "            state[0][y][x] = 1 #presence / volume\n",
    "            state[1][y][x] = car_speed #velocity\n",
    "            state[2][y][x] = traci.vehicle.getAccumulatedWaitingTime(car_id) #waitingTime\n",
    "\n",
    "        if 'W2TL' in lane_id:\n",
    "            x = int(lane_pos//7.5)\n",
    "            y = 99 + 3 + 3 - int(lane_id[-1])\n",
    "            state[0][y][x] = 1 #presence / volume\n",
    "            state[1][y][x] = car_speed #velocity\n",
    "            state[2][y][x] = traci.vehicle.getAccumulatedWaitingTime(car_id) #waitingTime\n",
    "\n",
    "    #Return a partial view of the state\n",
    "    return state[:, state.shape[1]//2 - 20: state.shape[1]//2 + 20, state.shape[2]//2 - 20: state.shape[2]//2 + 20].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23095f80-6aa4-495a-995e-785cd27880fe",
   "metadata": {},
   "source": [
    "# PPO Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d48996-d670-4bf2-b6f2-aee62a68c798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_iter(\n",
    "    epoch: int,\n",
    "    mini_batch_size: int,\n",
    "    states: torch.Tensor,\n",
    "    actions: torch.Tensor,\n",
    "    values: torch.Tensor,\n",
    "    log_probs: torch.Tensor,\n",
    "    returns: torch.Tensor,\n",
    "    advantages: torch.Tensor,\n",
    "):\n",
    "    \"\"\"Yield mini-batches.\"\"\"\n",
    "    batch_size = states.size(0)\n",
    "    for _ in range(epoch):\n",
    "        for _ in range(batch_size // mini_batch_size):\n",
    "            rand_ids = np.random.choice(batch_size, mini_batch_size)\n",
    "            yield states[rand_ids, :], actions[rand_ids], values[\n",
    "                rand_ids\n",
    "            ], log_probs[rand_ids], returns[rand_ids], advantages[rand_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770629aa-2470-4ec1-b613-da239b414d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent:\n",
    "    \"\"\"PPO Agent.\n",
    "    Attributes:\n",
    "        env (gym.Env): Gym env for training\n",
    "        gamma (float): discount factor\n",
    "        tau (float): lambda of generalized advantage estimation (GAE)\n",
    "        batch_size (int): batch size for sampling\n",
    "        epsilon (float): amount of clipping surrogate objective\n",
    "        epoch (int): the number of update\n",
    "        rollout_len (int): the number of rollout\n",
    "        entropy_weight (float): rate of weighting entropy into the loss function\n",
    "        actor (nn.Module): target actor model to select actions\n",
    "        critic (nn.Module): critic model to predict state values\n",
    "        transition (list): temporory storage for the recent transition\n",
    "        device (torch.device): cpu / gpu\n",
    "        total_step (int): total step numbers\n",
    "        is_test (bool): flag to show the current mode (train / test)        \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: gym.Env,\n",
    "        batch_size: int,\n",
    "        gamma: float,\n",
    "        tau: float,\n",
    "        epsilon: float,\n",
    "        epoch: int,\n",
    "        rollout_len: int,\n",
    "        entropy_weight: float,\n",
    "    ):\n",
    "        \"\"\"Initialize.\"\"\"\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon = epsilon\n",
    "        self.epoch = epoch\n",
    "        self.rollout_len = rollout_len\n",
    "        self.entropy_weight = entropy_weight\n",
    "\n",
    "        # device: cpu / gpu\n",
    "        self.device = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        print(self.device)\n",
    "\n",
    "        # networks\n",
    "        obs_dim = env.observation_space.shape[0]\n",
    "        action_dim = env.action_space.shape[0]\n",
    "        self.actor = Actor(obs_dim, action_dim).to(self.device)\n",
    "        self.critic = Critic(obs_dim).to(self.device)\n",
    "\n",
    "        # optimizer\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=0.001)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=0.005)\n",
    "\n",
    "        # memory for training\n",
    "        self.states: List[torch.Tensor] = []\n",
    "        self.actions: List[torch.Tensor] = []\n",
    "        self.rewards: List[torch.Tensor] = []\n",
    "        self.values: List[torch.Tensor] = []\n",
    "        self.masks: List[torch.Tensor] = []\n",
    "        self.log_probs: List[torch.Tensor] = []\n",
    "\n",
    "        # total steps count\n",
    "        self.total_step = 1\n",
    "\n",
    "        # mode: train / test\n",
    "        self.is_test = False\n",
    "\n",
    "    def select_action(self, state: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Select an action from the input state.\"\"\"\n",
    "        state = torch.FloatTensor(state).to(self.device)\n",
    "        action, dist = self.actor(state)\n",
    "        selected_action = dist.mean if self.is_test else action\n",
    "\n",
    "        if not self.is_test:\n",
    "            value = self.critic(state)\n",
    "            self.states.append(state)\n",
    "            self.actions.append(selected_action)\n",
    "            self.values.append(value)\n",
    "            self.log_probs.append(dist.log_prob(selected_action))\n",
    "\n",
    "        return selected_action.cpu().detach().numpy()\n",
    "\n",
    "    def step(self, action: np.ndarray) -> Tuple[np.ndarray, np.float64, bool]:\n",
    "        \"\"\"Take an action and return the response of the env.\"\"\"\n",
    "        next_state, reward, done, _, _ = self.env.step(action)\n",
    "        #values = self.env.step(action)\n",
    "        #print(len(values))\n",
    "        #print(values)\n",
    "        next_state = np.reshape(next_state, (1, -1)).astype(np.float64)\n",
    "        reward = np.reshape(reward, (1, -1)).astype(np.float64)\n",
    "        done = np.reshape(done, (1, -1))\n",
    "\n",
    "        if not self.is_test:\n",
    "            self.rewards.append(torch.FloatTensor(reward).to(self.device))\n",
    "            self.masks.append(torch.FloatTensor(1 - done).to(self.device))\n",
    "\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def update_model(\n",
    "        self, next_state: np.ndarray\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Update the model by gradient descent.\"\"\"\n",
    "        device = self.device  # for shortening the following lines\n",
    "\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        next_value = self.critic(next_state)\n",
    "\n",
    "        returns = compute_gae(\n",
    "            next_value,\n",
    "            self.rewards,\n",
    "            self.masks,\n",
    "            self.values,\n",
    "            self.gamma,\n",
    "            self.tau,\n",
    "        )\n",
    "\n",
    "        states = torch.cat(self.states).view(-1, 3)\n",
    "        actions = torch.cat(self.actions)\n",
    "        returns = torch.cat(returns).detach()\n",
    "        values = torch.cat(self.values).detach()\n",
    "        log_probs = torch.cat(self.log_probs).detach()\n",
    "        advantages = returns - values\n",
    "\n",
    "        actor_losses, critic_losses = [], []\n",
    "\n",
    "        for state, action, old_value, old_log_prob, return_, adv in ppo_iter(\n",
    "            epoch=self.epoch,\n",
    "            mini_batch_size=self.batch_size,\n",
    "            states=states,\n",
    "            actions=actions,\n",
    "            values=values,\n",
    "            log_probs=log_probs,\n",
    "            returns=returns,\n",
    "            advantages=advantages,\n",
    "        ):\n",
    "            # calculate ratios\n",
    "            _, dist = self.actor(state)\n",
    "            log_prob = dist.log_prob(action)\n",
    "            ratio = (log_prob - old_log_prob).exp()\n",
    "\n",
    "            # actor_loss\n",
    "            surr_loss = ratio * adv\n",
    "            clipped_surr_loss = (\n",
    "                torch.clamp(ratio, 1.0 - self.epsilon, 1.0 + self.epsilon) * adv\n",
    "            )\n",
    "\n",
    "            # entropy\n",
    "            entropy = dist.entropy().mean()\n",
    "\n",
    "            actor_loss = (\n",
    "                -torch.min(surr_loss, clipped_surr_loss).mean()\n",
    "                - entropy * self.entropy_weight\n",
    "            )\n",
    "\n",
    "            # critic_loss\n",
    "            value = self.critic(state)\n",
    "            #clipped_value = old_value + (value - old_value).clamp(-0.5, 0.5)\n",
    "            critic_loss = (return_ - value).pow(2).mean()\n",
    "        \n",
    "            # train critic\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            critic_loss.backward(retain_graph=True)\n",
    "            self.critic_optimizer.step()\n",
    "\n",
    "            # train actor\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "\n",
    "            actor_losses.append(actor_loss.item())\n",
    "            critic_losses.append(critic_loss.item())\n",
    "\n",
    "        self.states, self.actions, self.rewards = [], [], []\n",
    "        self.values, self.masks, self.log_probs = [], [], []\n",
    "\n",
    "        actor_loss = sum(actor_losses) / len(actor_losses)\n",
    "        critic_loss = sum(critic_losses) / len(critic_losses)\n",
    "\n",
    "        return actor_loss, critic_loss\n",
    "\n",
    "    def train(self, num_frames: int, plotting_interval: int = 200):\n",
    "        \"\"\"Train the agent.\"\"\"\n",
    "        self.is_test = False\n",
    "\n",
    "        state = self.env.reset()\n",
    "        #print(state)\n",
    "        state = np.expand_dims(state[0], axis=0)\n",
    "\n",
    "        actor_losses, critic_losses = [], []\n",
    "        scores = []\n",
    "        score = 0\n",
    "\n",
    "        while self.total_step <= num_frames + 1:\n",
    "            for _ in range(self.rollout_len):\n",
    "                self.total_step += 1\n",
    "                action = self.select_action(state)\n",
    "                #print(action)\n",
    "                next_state, reward, done = self.step(action)\n",
    "                #print(next_state)\n",
    "\n",
    "                state = next_state\n",
    "                state = np.expand_dims(state[0], axis=0)\n",
    "                score += reward[0][0]\n",
    "\n",
    "                # if episode ends\n",
    "                if done[0][0]:\n",
    "                    state = env.reset()\n",
    "                    state = np.expand_dims(state[0], axis=0)\n",
    "                    scores.append(score)\n",
    "                    score = 0\n",
    "\n",
    "                    self._plot(\n",
    "                        self.total_step, scores, actor_losses, critic_losses\n",
    "                    )\n",
    "\n",
    "            actor_loss, critic_loss = self.update_model(next_state)\n",
    "            actor_losses.append(actor_loss)\n",
    "            critic_losses.append(critic_loss)\n",
    "\n",
    "        # termination\n",
    "        self.env.close()\n",
    "\n",
    "    def test(self):\n",
    "        \"\"\"Test the agent.\"\"\"\n",
    "        self.is_test = True\n",
    "\n",
    "        state = self.env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "\n",
    "        frames = []\n",
    "        while not done:\n",
    "            frames.append(self.env.render())\n",
    "            action = self.select_action(state)\n",
    "            next_state, reward, done = self.step(action)\n",
    "\n",
    "            state = next_state\n",
    "            score += reward\n",
    "\n",
    "        print(\"score: \", score)\n",
    "        self.env.close()\n",
    "\n",
    "        return frames\n",
    "\n",
    "    def _plot(\n",
    "        self,\n",
    "        frame_idx: int,\n",
    "        scores: List[float],\n",
    "        actor_losses: List[float],\n",
    "        critic_losses: List[float],\n",
    "    ):\n",
    "        \"\"\"Plot the training progresses.\"\"\"\n",
    "\n",
    "        def subplot(loc: int, title: str, values: List[float]):\n",
    "            plt.subplot(loc)\n",
    "            plt.title(title)\n",
    "            plt.plot(values)\n",
    "\n",
    "        subplot_params = [\n",
    "            (131, f\"frame {frame_idx}. score: {np.mean(scores[-10:])}\", scores),\n",
    "            (132, \"actor_loss\", actor_losses),\n",
    "            (133, \"critic_loss\", critic_losses),\n",
    "        ]\n",
    "\n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=(30, 5))\n",
    "        for loc, title, values in subplot_params:\n",
    "            subplot(loc, title, values)\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
