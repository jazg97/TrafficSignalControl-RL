{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0849aba1-93c3-436b-b86e-8b1217186cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import copy\n",
    "import math\n",
    "from torch.distributions import Categorical\n",
    "from datetime import datetime\n",
    "import gymnasium as gym\n",
    "import os, shutil\n",
    "import argparse\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd \n",
    "import torch.optim as optim\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "from shutil import copyfile\n",
    "import sys\n",
    "import traci\n",
    "import random\n",
    "import timeit\n",
    "\n",
    "from generator import TrafficGenerator\n",
    "from memory import Memory     ## Prority Experience Memory \n",
    "from visualization import Visualization\n",
    "\n",
    "from utils import import_train_configuration,set_sumo, set_train_path,get_model_path\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a754b04-1cb7-4264-9385-c3214583b5e0",
   "metadata": {},
   "source": [
    "# Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93a51825-761e-44b1-afe7-c7d9c626c648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# phase codes based on SUMO environment.net.xml \n",
    "PHASE_NS_GREEN = 0  # action 0 for Variable Order\n",
    "PHASE_NS_YELLOW = 1\n",
    "PHASE_NSL_GREEN = 2  # action 1 for Variable Order\n",
    "PHASE_NSL_YELLOW = 3\n",
    "PHASE_EW_GREEN = 4  # action 2 for Variable Order\n",
    "PHASE_EW_YELLOW = 5\n",
    "PHASE_EWL_GREEN = 6  # action 3 for Variable Order\n",
    "PHASE_EWL_YELLOW = 7\n",
    "\n",
    "# New phases added\n",
    "PHASE_N_SL_GREEN = 8\n",
    "PHASE_N_SL_YELLOW= 9\n",
    "PHASE_E_SL_GREEN = 10\n",
    "PHASE_E_SL_YELLOW= 11\n",
    "PHASE_S_SL_GREEN = 12\n",
    "PHASE_S_SL_YELLOW= 13\n",
    "PHASE_W_SL_GREEN = 14\n",
    "PHASE_W_SL_YELLOW= 15"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c1e7af5f-db8c-4e4e-9943-8848158cf199",
   "metadata": {},
   "source": [
    "#Duration_NS = 12\n",
    "#Duration_NSL = 9\n",
    "#Duration_EW = 16\n",
    "#Duration_EWL = 7\n",
    "\n",
    "Duration_NS = 18\n",
    "Duration_NSL = 7\n",
    "Duration_EW = 16\n",
    "Duration_EWL = 7\n",
    "\n",
    "Duration_N_SL = 7\n",
    "Duration_E_SL = 7\n",
    "Duration_S_SL = 7\n",
    "Duration_W_SL = 7\n",
    "\n",
    "# Duration_NS = 30\n",
    "# Duration_NSL = 11\n",
    "# Duration_EW = 22\n",
    "# Duration_EWL = 12\n",
    "\n",
    "O = [Duration_NS,Duration_NSL,Duration_EW,Duration_EWL]\n",
    "duration = [Duration_NS,Duration_NSL,Duration_EW,Duration_EWL]\n",
    "old_duration = [Duration_NS,Duration_NSL,Duration_EW,Duration_EWL]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d678bed-373e-40de-bec4-d499becb4e08",
   "metadata": {},
   "source": [
    "# State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c3f5978-8676-400b-8462-ac1258b875b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_state():\n",
    "    \"\"\"\n",
    "    Retrieve the state of the intersection from sumo, in the form of cell occupancy\n",
    "    \"\"\"\n",
    "    state = np.zeros((3, 208, 206))   ## kind of like an RGB image\n",
    "    lane = [\"N2TL_0\",\"N2TL_1\",\"N2TL_2\",\"E2TL_0\",\"E2TL_1\",\"E2TL_2\",\"E2TL_3\",\"S2TL_0\",\"S2TL_1\",\"S2TL_2\",\"W2TL_0\",\"W2TL_1\",\"W2TL_2\",\"W2TL_3\"]\n",
    "    # N, E, S, W\n",
    "    #           N\n",
    "    #   W               E\n",
    "    #           S    \n",
    "    car_list = traci.vehicle.getIDList()\n",
    "\n",
    "    for car_id in car_list:\n",
    "        lane_pos = traci.vehicle.getLanePosition(car_id)\n",
    "        car_speed = traci.vehicle.getSpeed(car_id)\n",
    "        lane_id = traci.vehicle.getLaneID(car_id)\n",
    "\n",
    "        if 'N2TL' in lane_id:            \n",
    "            x = 100 + int(lane_id[-1])\n",
    "            y = int(lane_pos//7.5)\n",
    "            state[0][y][x] = 1 #presence / volume\n",
    "            state[1][y][x] = car_speed / 50.0 #velocity\n",
    "            state[2][y][x] = traci.vehicle.getAccumulatedWaitingTime(car_id)#waitingTime\n",
    "            \n",
    "        if 'E2TL' in lane_id:\n",
    "            x = 205 - int(lane_pos//7.5)\n",
    "            y = 99 + int(lane_id[-1])\n",
    "            state[0][y][x] = 1 #presence / volume\n",
    "            state[1][y][x] = car_speed / 50.0#velocity\n",
    "            state[2][y][x] = traci.vehicle.getAccumulatedWaitingTime(car_id) #waitingTime\n",
    "\n",
    "        if 'S2TL' in lane_id:\n",
    "            x = 100 + 3 + int(lane_id[-1])\n",
    "            y = 207 - int(lane_pos//7.5)\n",
    "            state[0][y][x] = 1 #presence / volume\n",
    "            state[1][y][x] = car_speed / 50.0#velocity\n",
    "            state[2][y][x] = traci.vehicle.getAccumulatedWaitingTime(car_id) #waitingTime\n",
    "\n",
    "        if 'W2TL' in lane_id:\n",
    "            x = int(lane_pos//7.5)\n",
    "            y = 99 + 4 + 3 - int(lane_id[-1])\n",
    "            state[0][y][x] = 1 #presence / volume\n",
    "            state[1][y][x] = car_speed /50.0#velocity\n",
    "            state[2][y][x] = traci.vehicle.getAccumulatedWaitingTime(car_id) #waitingTime\n",
    "\n",
    "    #Return a partial view of the state\n",
    "    return state[:, state.shape[1]//2 - 24: state.shape[1]//2 + 24, state.shape[2]//2 - 23: state.shape[2]//2 + 23]#.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fb9b4c-3cdf-4b44-be39-2acfc163ae9e",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50121058-29b5-472c-b283-1323c3ff7683",
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent,trafficGen,sumo_cmd,opt.max_e_steps,green_duration,yellow_duration,opt.state_dim,opt.action_dim, False\n",
    "class Simulation:\n",
    "    def __init__(self, Agent, TrafficGen, sumo_cmd, max_steps, green_duration, yellow_duration, num_states, num_actions, mode, device):\n",
    "        #self._model = Model\n",
    "        #self._Model_A = Model.critic\n",
    "        #self._Model_B = Model.critic_target\n",
    "        self._Agent = Agent\n",
    "        self._Actor = Agent.actor\n",
    "        self._Critic= Agent.critic\n",
    "        #self._Memory = Memory\n",
    "        self._TrafficGen = TrafficGen\n",
    "        #self._gamma = gamma\n",
    "        self._step = 0\n",
    "        self._sumo_cmd = sumo_cmd\n",
    "        self._max_steps = max_steps \n",
    "        self._green_duration = green_duration\n",
    "        self._yellow_duration = yellow_duration\n",
    "        self._num_states = num_states\n",
    "        self._num_actions = num_actions\n",
    "        self._reward_store = []\n",
    "        self._speed_store = []\n",
    "        self._cumulative_wait_store = []\n",
    "        self._avg_queue_length_store = []\n",
    "        self._eval = mode\n",
    "        self.dvc = device\n",
    "\n",
    "    def run(self, episode):\n",
    "        \"\"\"\n",
    "        Runs an episode of simulation, then starts a training session\n",
    "        \"\"\"\n",
    "        self.training = False\n",
    "        start_time = timeit.default_timer()\n",
    "\n",
    "        # first, generate the route file for this simulation and set up sumo\n",
    "        if not self._eval:\n",
    "            self._TrafficGen.generate_routefile(seed=episode)\n",
    "        else:\n",
    "            self._TrafficGen.generate_routefile(seed=400+episode)\n",
    "        traci.start(self._sumo_cmd)\n",
    "        print(\"Simulating...\")\n",
    "\n",
    "        # inits\n",
    "        self._step = 0\n",
    "        self._waiting_times = {}\n",
    "        self._sum_neg_reward = 0\n",
    "        self._sum_queue_length = 0\n",
    "        self._sum_waiting_time = 0\n",
    "        duration = [0,0,0,0,0,0,0,0]\n",
    "        self._sum_speed = 0\n",
    "        reward = 0\n",
    "        re = 0\n",
    "        current_phase = 0\n",
    "        self.reward = 0\n",
    "        done = 0\n",
    "        old_action = 0\n",
    "        \n",
    "        self._simulate(50)  ## Warm Environment\n",
    "        while self._step < self._max_steps:\n",
    "            \n",
    "            # get current state of the intersection\n",
    "            current_state = _get_state()\n",
    "\n",
    "            # calculate reward of previous action: \n",
    "            # waiting time = seconds waited by a car since the spawn in the environment, cumulated for every car in incoming lanes\n",
    "            current_total_wait = self._collect_waiting_times()    \n",
    "            \n",
    "            ## Your Reward Function \n",
    "            reward = -self._get_queue_length()                \n",
    "            # Current light phase\n",
    "            current_phase = int(traci.trafficlight.getPhase(\"TL\")/2)\n",
    "            # Chosen action\n",
    "            action, logprob_a = self._choose_action(current_state, self._eval) # phase, epsilon) \n",
    "            \n",
    "            \n",
    "            # saving the data into the memory                \n",
    "            # if the chosen phase is different from the last phase, activate the yellow phase\n",
    "            if self._step != 0 and old_action != action:# and i == 0:\n",
    "                self._set_yellow_phase(current_phase)\n",
    "                self._simulate(self._yellow_duration)\n",
    "                duration[action] = self._green_duration\n",
    "            else: \n",
    "                duration[action] += 7\n",
    "\n",
    "            if self._step != 0:\n",
    "                next_state = _get_state()\n",
    "                if self._step < self._max_steps - self._green_duration - self._yellow_duration:\n",
    "                    done = 0\n",
    "                else:\n",
    "                    done = 1\n",
    "                if not self._eval and agent.idx < agent.T_horizon:\n",
    "                    self._Agent.put_data(current_state, action, reward, next_state, logprob_a, done, done)\n",
    "            self._set_green_phase(action)\n",
    "            self._simulate(self._green_duration)\n",
    "            # print(\"current phase:\",current_phase,\"green:\",a)\n",
    "\n",
    "            # saving only the meaningful reward to better see if the agent is behaving correctly\n",
    "            # if reward < 0:\n",
    "            self._sum_neg_reward += reward\n",
    "            re += 1\n",
    "            self.reward = self._sum_neg_reward/re\n",
    "            old_action = action\n",
    "                \n",
    "        print(\"Total Queue:\",self._sum_queue_length, \"  \", \"Average Reward:\", self.reward, \" \", \"Average Speed:\",self._sum_speed/self._max_steps)\n",
    "\n",
    "        self._save_episode_stats()\n",
    "        print(\"Total Reward:\", self._sum_neg_reward)\n",
    "        traci.close()\n",
    "        simulation_time = round(timeit.default_timer() - start_time, 1)\n",
    "        if not self._eval: \n",
    "            return simulation_time\n",
    "        else:\n",
    "            return simulation_time, self.reward\n",
    "\n",
    "    def _simulate(self, steps_todo):\n",
    "        \"\"\"\n",
    "        Execute steps in sumo while gathering statistics\n",
    "        \"\"\"\n",
    "        if (self._step + steps_todo) >= self._max_steps:  # do not do more steps than the maximum allowed number of steps\n",
    "            steps_todo = self._max_steps - self._step\n",
    "\n",
    "        while steps_todo > 0:\n",
    "            traci.simulationStep()  # simulate 1 step in sumo\n",
    "            self._step += 1 # update the step counter\n",
    "            steps_todo -= 1\n",
    "            queue_length = self._get_queue_length()\n",
    "            self._sum_queue_length += queue_length\n",
    "            self._sum_waiting_time += queue_length # 1 step while wating in queue means 1 second waited, for each car, therefore queue_lenght == waited_seconds\n",
    "            speed = self._get_speed()\n",
    "            self._sum_speed += speed\n",
    "        \n",
    "    def _collect_waiting_times(self):            # For reward \n",
    "        \"\"\"\n",
    "        Retrieve the waiting time of every car in the incoming roads\n",
    "        \"\"\"\n",
    "        incoming_roads = [\"E2TL\", \"N2TL\", \"W2TL\", \"S2TL\"]\n",
    "        car_list = traci.vehicle.getIDList()\n",
    "        self._waiting_times = {}\n",
    "        for car_id in car_list:\n",
    "            wait_time = traci.vehicle.getAccumulatedWaitingTime(car_id)\n",
    "            road_id = traci.vehicle.getRoadID(car_id)  # get the road id where the car is located\n",
    "            if road_id in incoming_roads:  # consider only the waiting times of cars in incoming roads\n",
    "                self._waiting_times[car_id] = wait_time \n",
    "            else:\n",
    "                if car_id in self._waiting_times: # a car that was tracked has cleared the intersection\n",
    "                    del self._waiting_times[car_id] \n",
    "                \n",
    "        if len(self._waiting_times) == 0: \n",
    "            total_waiting_time = 0\n",
    "        else: \n",
    "            total_waiting_time = sum(self._waiting_times.values())/len(self._waiting_times)\n",
    "        return total_waiting_time \n",
    "    '''\n",
    "    def _choose_action(self, state, epsilon,phase,old_duration): #phase, epsilon):\n",
    "        \"\"\"\n",
    "        Decide wheter to perform an explorative or exploitative action, according to an epsilon-greedy policy\n",
    "        \"\"\"\n",
    "        if random.random() < epsilon:\n",
    "            return random.randint(0, self._num_actions - 1) # random action\n",
    "        else:\n",
    "            x = [0,0,0,0,0,0,0,0]\n",
    "            x[phase] = 1\n",
    "            return torch.argmax(self._model.predict(state,x,old_duration))#,phase)) # the best action given the current state\n",
    "    '''\n",
    "    def _choose_action(self, state, deterministic):\n",
    "        state = torch.from_numpy(state).float().to(self.dvc)\n",
    "        #print(state.shape)\n",
    "        with torch.no_grad():\n",
    "            pi = self._Actor.pi(state, softmax_dim=0)\n",
    "            #print(pi)\n",
    "            #print(pi.shape)\n",
    "            if deterministic:\n",
    "                action = torch.argmax(pi).item()\n",
    "                return action, None\n",
    "            else:\n",
    "                m = Categorical(pi)\n",
    "                #print(m)\n",
    "                action = m.sample().item()\n",
    "                pi_a = pi[0,action].item()\n",
    "                return action, pi_a\n",
    "\n",
    "    def _set_yellow_phase(self, old_action):\n",
    "        \"\"\"\n",
    "        Activate the correct yellow light combination in sumo\n",
    "        \"\"\"\n",
    "        yellow_phase_code = old_action * 2 + 1 # obtain the yellow phase code, based on the old action (ref on environment.net.xml)\n",
    "        traci.trafficlight.setPhase(\"TL\", yellow_phase_code)\n",
    "\n",
    "    def _set_green_phase(self, action_number):   ## For Variable Order Method \n",
    "        \"\"\"\n",
    "        Activate the correct green light combination in sumo\n",
    "        \"\"\"\n",
    "        if action_number == 0:\n",
    "            traci.trafficlight.setPhase(\"TL\", PHASE_NS_GREEN)\n",
    "        elif action_number == 1:\n",
    "            traci.trafficlight.setPhase(\"TL\", PHASE_NSL_GREEN)\n",
    "        elif action_number == 2:\n",
    "            traci.trafficlight.setPhase(\"TL\", PHASE_EW_GREEN)\n",
    "        elif action_number == 3:\n",
    "            traci.trafficlight.setPhase(\"TL\", PHASE_EWL_GREEN)\n",
    "        elif action_number == 4:\n",
    "            traci.trafficlight.setPhase('TL', PHASE_N_SL_GREEN)\n",
    "        elif action_number == 5:\n",
    "            traci.trafficlight.setPhase('TL', PHASE_E_SL_GREEN)\n",
    "        elif action_number == 6:\n",
    "            traci.trafficlight.setPhase('TL', PHASE_S_SL_GREEN)\n",
    "        elif action_number == 7:\n",
    "            traci.trafficlight.setPhase('TL', PHASE_W_SL_GREEN)\n",
    "\n",
    "        # Add New phases (North Straight and Left, South Straight and Left, West Straight and Left, East Straight and Left)\n",
    "\n",
    "    def _get_green(self,current_phase):       ## For Finetuning Method \n",
    "        if current_phase == 0:\n",
    "            green = Duration_NS\n",
    "        elif current_phase == 1:\n",
    "            green = Duration_NSL\n",
    "        elif current_phase == 2:\n",
    "            green = Duration_EW\n",
    "        elif current_phase == 3: \n",
    "            green = Duration_EWL\n",
    "        else:\n",
    "            green = Duration_N_SL\n",
    "        \n",
    "        return green\n",
    "\n",
    "    def _get_queue_length(self):          # For evaluation \n",
    "        \"\"\"\n",
    "        Retrieve the number of cars with speed = 0 in every incoming lane\n",
    "        \"\"\"\n",
    "        halt_N = traci.edge.getLastStepHaltingNumber(\"N2TL\")\n",
    "        halt_S = traci.edge.getLastStepHaltingNumber(\"S2TL\")\n",
    "        halt_E = traci.edge.getLastStepHaltingNumber(\"E2TL\")\n",
    "        halt_W = traci.edge.getLastStepHaltingNumber(\"W2TL\")\n",
    "        queue_length = halt_N + halt_S + halt_E + halt_W\n",
    "        return queue_length\n",
    "    \n",
    "    def _get_speed(self):                  # For evaluation \n",
    "        total_speed = 0\n",
    "        car_list = traci.vehicle.getIDList()\n",
    "        for car_id in car_list:\n",
    "            car_speed = traci.vehicle.getSpeed(car_id)\n",
    "            total_speed +=car_speed\n",
    "        if len(car_list) == 0: \n",
    "            s = 0\n",
    "        else: \n",
    "            s = total_speed/len(car_list)\n",
    "        return s\n",
    "            \n",
    "    def _save_episode_stats(self):\n",
    "        \"\"\"\n",
    "        Save the stats of the episode to plot the graphs at the end of the session\n",
    "        \"\"\"\n",
    "        self._reward_store.append(self.reward)  # how much negative reward in this episode\n",
    "        self._speed_store.append(self._sum_speed / self._max_steps)\n",
    "        self._cumulative_wait_store.append(self._sum_waiting_time)  # total number of seconds waited by cars in this episode\n",
    "        self._avg_queue_length_store.append(self._sum_queue_length / self._max_steps)  # average number of queued cars per step, in this episode\n",
    "    \n",
    "    @property\n",
    "    def reward_store(self):\n",
    "        return self._reward_store\n",
    "\n",
    "    @property\n",
    "    def speed_store(self):\n",
    "        return self._speed_store\n",
    "    \n",
    "    @property\n",
    "    def cumulative_wait_store(self):\n",
    "        return self._cumulative_wait_store\n",
    "\n",
    "    @property\n",
    "    def avg_queue_length_store(self):\n",
    "        return self._avg_queue_length_store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5339685d-5d8b-412d-89ac-10d9cfa4c569",
   "metadata": {},
   "source": [
    "# Actor-Critic Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1849e8b6-ee03-47f6-a678-dd9c51dfc743",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, action_dim):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, (5,5), (1,1), padding='same')\n",
    "        self.pool1 = nn.MaxPool2d((2, 2), stride=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, (3,3), (1,1), padding='same')\n",
    "        self.pool2 = nn.MaxPool2d((2, 2), stride=2)\n",
    "        self.l1 = nn.Linear(32*12*11, 2048)\n",
    "        self.l2 = nn.Linear(2048, 512)\n",
    "        self.l3 = nn.Linear(512, action_dim)        \n",
    "\n",
    "    def forward(self, state):\n",
    "        n = self.pool2(F.relu(self.conv2(self.pool1(F.relu(self.conv1(state))))))\n",
    "        n = n.reshape(-1, 32*11*12)\n",
    "        n = torch.tanh(self.l1(n))\n",
    "        n = torch.tanh(self.l2(n))\n",
    "        return n\n",
    "\n",
    "    def pi(self, state, softmax_dim = 0):\n",
    "        n = self.forward(state)\n",
    "        prob = F.softmax(self.l3(n), dim=softmax_dim)\n",
    "        return prob\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, (5,5), (1,1), padding='same')\n",
    "        self.pool1 = nn.MaxPool2d((2, 2), stride=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, (3,3), (1,1), padding='same')\n",
    "        self.pool2 = nn.MaxPool2d((2, 2), stride=2)\n",
    "        self.C1 = nn.Linear(32*12*11, 2048)\n",
    "        self.C2 = nn.Linear(2048, 512)\n",
    "        self.C3 = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        v = self.pool2(F.relu(self.conv2(self.pool1(F.relu(self.conv1(state))))))\n",
    "        v = v.reshape(-1, 32*12*11)\n",
    "        v = torch.relu(self.C1(v))\n",
    "        v = torch.relu(self.C2(v))\n",
    "        v = self.C3(v)\n",
    "        return v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f35f67-7734-45fb-a86f-37e80dc61af5",
   "metadata": {},
   "source": [
    "# Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abc8eb39-fae7-4988-aadf-a36af1917e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# Need to update to SUMO environment\n",
    "def evaluate_policy(env, agent, turns = 3):\n",
    "    total_scores = 0\n",
    "    total_time = 0\n",
    "    for j in range(turns):\n",
    "        episode = random.randint(301, 2**31 - 1)\n",
    "        simulation_time, reward = env.run(episode)\n",
    "        total_scores += reward\n",
    "        total_time += simulation_time\n",
    "    return int(total_scores/turns), total_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524dd35f-4839-4ca6-96db-7d7c62881811",
   "metadata": {},
   "source": [
    "# PPO Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e018799-968c-484a-bf8d-cafd74a096cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to update to SUMO environment\n",
    "class PPO_agent():\n",
    "    def __init__(self, **kwargs):\n",
    "        # Init hyperparameters for PPO agent, just like \"self.gamma = opt.gamma, self.lambd = opt.lambd, ...\"\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "        '''Build Actor and Critic'''\n",
    "        self.actor = Actor(self.action_dim).to(self.dvc)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=self.lr)\n",
    "        self.critic = Critic().to(self.dvc)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=self.lr)\n",
    "\n",
    "        '''Build Trajectory holder'''\n",
    "        self.s_hoder = np.zeros(([self.T_horizon] + self.state_dim), dtype=np.float32)\n",
    "        self.a_hoder = np.zeros((self.T_horizon, 1), dtype=np.int64)\n",
    "        self.r_hoder = np.zeros((self.T_horizon, 1), dtype=np.float32)\n",
    "        self.s_next_hoder = np.zeros(([self.T_horizon] + self.state_dim), dtype=np.float32)\n",
    "        self.logprob_a_hoder = np.zeros((self.T_horizon, 1), dtype=np.float32)\n",
    "        self.done_hoder = np.zeros((self.T_horizon, 1), dtype=np.bool_)\n",
    "        self.dw_hoder = np.zeros((self.T_horizon, 1), dtype=np.bool_)\n",
    "        self.idx = 0\n",
    "\n",
    "    def train(self):\n",
    "        start_time = timeit.default_timer()\n",
    "        self.entropy_coef *= self.entropy_coef_decay #exploring decay\n",
    "        '''Prepare PyTorch data from Numpy data'''\n",
    "        s = torch.from_numpy(self.s_hoder).to(self.dvc)\n",
    "        a = torch.from_numpy(self.a_hoder).to(self.dvc)\n",
    "        r = torch.from_numpy(self.r_hoder).to(self.dvc)\n",
    "        s_next = torch.from_numpy(self.s_next_hoder).to(self.dvc)\n",
    "        old_prob_a = torch.from_numpy(self.logprob_a_hoder).to(self.dvc)\n",
    "        done = torch.from_numpy(self.done_hoder).to(self.dvc)\n",
    "        dw = torch.from_numpy(self.dw_hoder).to(self.dvc)\n",
    "\n",
    "        ''' Use TD+GAE+LongTrajectory to compute Advantage and TD target'''\n",
    "        with torch.no_grad():\n",
    "            vs = self.critic(s)\n",
    "            vs_ = self.critic(s_next)\n",
    "\n",
    "            '''dw(dead and win) for TD_target and Adv'''\n",
    "            deltas = r + self.gamma * vs_ * (~dw) - vs\n",
    "            deltas = deltas.cpu().flatten().numpy()\n",
    "            adv = [0]\n",
    "\n",
    "            '''done for GAE'''\n",
    "            for dlt, done in zip(deltas[::-1], done.cpu().flatten().numpy()[::-1]):\n",
    "                advantage = dlt + self.gamma * self.lambd * adv[-1] * (~done)\n",
    "                adv.append(advantage)\n",
    "            adv.reverse()\n",
    "            adv = copy.deepcopy(adv[0:-1])\n",
    "            adv = torch.tensor(adv).unsqueeze(1).float().to(self.dvc)\n",
    "            td_target = adv + vs\n",
    "            if self.adv_normalization:\n",
    "                adv = (adv - adv.mean()) / ((adv.std() + 1e-4))  #sometimes helps\n",
    "\n",
    "        \"\"\"PPO update\"\"\"\n",
    "        #Slice long trajectopy into short trajectory and perform mini-batch PPO update\n",
    "        optim_iter_num = int(math.ceil(s.shape[0] / self.batch_size))\n",
    "\n",
    "        for _ in range(self.K_epochs):\n",
    "            #Shuffle the trajectory, Good for training\n",
    "            perm = np.arange(s.shape[0])\n",
    "            np.random.shuffle(perm)\n",
    "            perm = torch.LongTensor(perm).to(self.dvc)\n",
    "            s, a, td_target, adv, old_prob_a = \\\n",
    "                s[perm].clone(), a[perm].clone(), td_target[perm].clone(), adv[perm].clone(), old_prob_a[perm].clone()\n",
    "\n",
    "            '''mini-batch PPO update'''\n",
    "            for i in range(optim_iter_num):\n",
    "                index = slice(i * self.batch_size, min((i + 1) * self.batch_size, s.shape[0]))\n",
    "\n",
    "                '''actor update'''\n",
    "                prob = self.actor.pi(s[index], softmax_dim=1)\n",
    "                entropy = Categorical(prob).entropy().sum(0, keepdim=True)\n",
    "                prob_a = prob.gather(1, a[index])\n",
    "                ratio = torch.exp(torch.log(prob_a) - torch.log(old_prob_a[index]))  # a/b == exp(log(a)-log(b))\n",
    "\n",
    "                surr1 = ratio * adv[index]\n",
    "                surr2 = torch.clamp(ratio, 1 - self.clip_rate, 1 + self.clip_rate) * adv[index]\n",
    "                a_loss = -torch.min(surr1, surr2) - self.entropy_coef * entropy\n",
    "\n",
    "                self.actor_optimizer.zero_grad()\n",
    "                a_loss.mean().backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 40)\n",
    "                self.actor_optimizer.step()\n",
    "\n",
    "                '''critic update'''\n",
    "                c_loss = (self.critic(s[index]) - td_target[index]).pow(2).mean()\n",
    "                for name, param in self.critic.named_parameters():\n",
    "                    if 'weight' in name:\n",
    "                        c_loss += param.pow(2).sum() * self.l2_reg\n",
    "\n",
    "                self.critic_optimizer.zero_grad()\n",
    "                c_loss.backward()\n",
    "                self.critic_optimizer.step()\n",
    "        simulation_time = round(timeit.default_timer() - start_time, 1)\n",
    "        return simulation_time\n",
    "\n",
    "    def put_data(self, s, a, r, s_next, logprob_a, done, dw):\n",
    "        self.s_hoder[self.idx] = s\n",
    "        self.a_hoder[self.idx] = a\n",
    "        self.r_hoder[self.idx] = r\n",
    "        self.s_next_hoder[self.idx] = s_next\n",
    "        self.logprob_a_hoder[self.idx] = logprob_a\n",
    "        self.done_hoder[self.idx] = done\n",
    "        self.dw_hoder[self.idx] = dw\n",
    "        self.idx+=1\n",
    "\n",
    "    def save(self, episode):\n",
    "        torch.save(self.critic.state_dict(), \"./model/ppo_critic{}.pth\".format(episode))\n",
    "        torch.save(self.actor.state_dict(), \"./model/ppo_actor{}.pth\".format(episode))\n",
    "\n",
    "    def load(self, episode):\n",
    "        self.critic.load_state_dict(torch.load(\"./model/ppo_critic{}.pth\".format(episode)))\n",
    "        self.actor.load_state_dict(torch.load(\"./model/ppo_actor{}.pth\".format(episode)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b623d44-c73a-4301-9d11-203375aa145f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOOptions:\n",
    "    def __init__(self, dvc: str = 'cuda', EnvIndex: int = 0, render: bool = False, seed: int = 209, T_horizon: int = 2048,\n",
    "                 Max_train_steps: int = 5e7, eval_interval: int = 5e3, gamma: float = 0.99, lambd: float = 0.95, clip_rate: float = 0.2,\n",
    "                 K_epochs: int = 10, net_width: int = 64, lr: float = 1e-4, l2_reg: float = 0, batch_size: int = 64, entropy_coef: float = 0,\n",
    "                 entropy_coef_decay: float = 0.99, adv_normalization: bool = False):\n",
    "\n",
    "        self.dvc = dvc\n",
    "        self.EnvIdex = EnvIndex\n",
    "        self.render = render\n",
    "        self.seed = seed\n",
    "        self.T_horizon = T_horizon\n",
    "        self.Max_train_steps = Max_train_steps\n",
    "        self.eval_interval = eval_interval\n",
    "        self.gamma = gamma\n",
    "        self.lambd = lambd\n",
    "        self.clip_rate = clip_rate\n",
    "        self.K_epochs = K_epochs\n",
    "        self.net_width = net_width\n",
    "        self.lr = lr\n",
    "        self.l2_reg = l2_reg\n",
    "        self.batch_size = batch_size\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.entropy_coef_decay = entropy_coef_decay\n",
    "        self.adv_normalization = adv_normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bad5cd5-dc72-4433-b93d-be528f1e3caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Episode 1 of 100\n",
      "0 2500\n",
      "Simulating...\n",
      "Total Queue: 70113    Average Reward: -19.397959183673468   Average Speed: 7.749328090398021\n",
      "Total Reward: -5703\n",
      "Simulation time: 124.7 s\n",
      "\n",
      "----- Episode 2 of 100\n",
      "294 2500\n",
      "Simulating...\n",
      "Total Queue: 67215    Average Reward: -18.761245674740483   Average Speed: 7.192748483489015\n",
      "Total Reward: -5422\n",
      "Simulation time: 122.3 s\n",
      "\n",
      "----- Episode 3 of 100\n",
      "583 2500\n",
      "Simulating...\n",
      "Total Queue: 96416    Average Reward: -26.697594501718214   Average Speed: 6.408162554112303\n",
      "Total Reward: -7769\n",
      "Simulation time: 124.7 s\n",
      "\n",
      "----- Episode 4 of 100\n",
      "874 2500\n",
      "Simulating...\n",
      "Total Queue: 103879    Average Reward: -28.781690140845072   Average Speed: 6.334582190641139\n",
      "Total Reward: -8174\n",
      "Simulation time: 124.9 s\n",
      "\n",
      "----- Episode 5 of 100\n",
      "1158 2500\n",
      "Simulating...\n",
      "Total Queue: 66437    Average Reward: -18.439446366782008   Average Speed: 7.489473124938892\n",
      "Total Reward: -5329\n",
      "Simulation time: 122.0 s\n",
      "\n",
      "----- Episode 6 of 100\n",
      "1447 2500\n",
      "Simulating...\n",
      "Total Queue: 58726    Average Reward: -15.97594501718213   Average Speed: 8.151346361549606\n",
      "Total Reward: -4649\n",
      "Simulation time: 121.6 s\n",
      "\n",
      "----- Episode 7 of 100\n",
      "1738 2500\n",
      "Simulating...\n",
      "Total Queue: 71766    Average Reward: -20.160958904109588   Average Speed: 7.5393414465226885\n",
      "Total Reward: -5887\n",
      "Simulation time: 122.8 s\n",
      "\n",
      "----- Episode 8 of 100\n",
      "2030 2500\n",
      "Simulating...\n",
      "Total Queue: 72508    Average Reward: -20.32986111111111   Average Speed: 7.75812815443799\n",
      "Total Reward: -5855\n",
      "Simulation time: 123.8 s\n",
      "\n",
      "----- Episode 9 of 100\n",
      "2318 2500\n",
      "Simulating...\n",
      "Total Queue: 73713    Average Reward: -20.601374570446737   Average Speed: 7.28710339540305\n",
      "Total Reward: -5995\n",
      "Simulation time: 123.1 s - Training time: 360.1 s - Total: 483.2 s\n",
      "\n",
      "----- Episode 10 of 100\n",
      "0 2500\n",
      "Simulating...\n",
      "Total Queue: 82767    Average Reward: -23.286206896551725   Average Speed: 7.2155489732191125\n",
      "Total Reward: -6753\n",
      "Simulation time: 124.4 s\n",
      "\n",
      "----- Episode 11 of 100\n",
      "290 2500\n",
      "Simulating...\n",
      "Total Queue: 74020    Average Reward: -20.343859649122805   Average Speed: 7.140789803979526\n",
      "Total Reward: -5798\n",
      "Simulation time: 123.2 s\n",
      "\n",
      "----- Episode 12 of 100\n",
      "575 2500\n",
      "Simulating...\n",
      "Total Queue: 76036    Average Reward: -21.114186851211073   Average Speed: 7.147066048552005\n",
      "Total Reward: -6102\n",
      "Simulation time: 123.9 s\n",
      "\n",
      "----- Episode 13 of 100\n",
      "864 2500\n",
      "Simulating...\n",
      "Total Queue: 68821    Average Reward: -18.84536082474227   Average Speed: 8.480657520174379\n",
      "Total Reward: -5484\n",
      "Simulation time: 122.4 s\n",
      "\n",
      "----- Episode 14 of 100\n",
      "1155 2500\n",
      "Simulating...\n",
      "Total Queue: 72383    Average Reward: -20.567010309278352   Average Speed: 8.508027428219108\n",
      "Total Reward: -5985\n",
      "Simulation time: 123.8 s\n",
      "\n",
      "----- Episode 15 of 100\n",
      "1446 2500\n",
      "Simulating...\n",
      "Total Queue: 111528    Average Reward: -31.626712328767123   Average Speed: 7.062012896215506\n",
      "Total Reward: -9235\n",
      "Simulation time: 126.0 s\n",
      "Simulating...\n",
      "Total Queue: 1389802    Average Reward: -391.1456692913386   Average Speed: 1.683646773528724\n",
      "Total Reward: -198702\n",
      "Simulating...\n",
      "Total Queue: 1371409    Average Reward: -385.9665354330709   Average Speed: 1.8127953148748124\n",
      "Total Reward: -196071\n",
      "Simulating...\n",
      "Total Queue: 1438146    Average Reward: -404.76181102362204   Average Speed: 1.7146247566496875\n",
      "Total Reward: -205619\n",
      "Evaluation time: 987.0 s Score: -393\n",
      "\n",
      "----- Episode 16 of 100\n",
      "1738 2500\n",
      "Simulating...\n",
      "Total Queue: 63017    Average Reward: -17.77054794520548   Average Speed: 8.062904354840407\n",
      "Total Reward: -5189\n",
      "Simulation time: 122.2 s\n",
      "\n",
      "----- Episode 17 of 100\n",
      "2030 2500\n",
      "Simulating...\n",
      "Total Queue: 62012    Average Reward: -16.97594501718213   Average Speed: 8.502360659425722\n",
      "Total Reward: -4940\n",
      "Simulation time: 121.8 s\n",
      "\n",
      "----- Episode 18 of 100\n",
      "2321 2500\n",
      "Simulating...\n",
      "Total Queue: 109632    Average Reward: -30.131034482758622   Average Speed: 6.733073001735014\n",
      "Total Reward: -8738\n",
      "Simulation time: 125.5 s - Training time: 362.1 s - Total: 487.6 s\n",
      "\n",
      "----- Episode 19 of 100\n",
      "0 2500\n",
      "Simulating...\n",
      "Total Queue: 70157    Average Reward: -19.64965986394558   Average Speed: 8.211108979784077\n",
      "Total Reward: -5777\n",
      "Simulation time: 125.6 s\n",
      "\n",
      "----- Episode 20 of 100\n",
      "294 2500\n",
      "Simulating...\n",
      "Total Queue: 84721    Average Reward: -23.654109589041095   Average Speed: 6.7874214823932135\n",
      "Total Reward: -6907\n",
      "Simulation time: 126.2 s\n",
      "\n",
      "----- Episode 21 of 100\n",
      "586 2500\n",
      "Simulating...\n",
      "Total Queue: 68987    Average Reward: -19.358885017421603   Average Speed: 8.446791912118844\n",
      "Total Reward: -5556\n",
      "Simulation time: 124.6 s\n",
      "\n",
      "----- Episode 22 of 100\n",
      "873 2500\n",
      "Simulating...\n"
     ]
    }
   ],
   "source": [
    "model_to_test = 1   # load model\n",
    "if __name__ == \"__main__\":\n",
    "    config = import_train_configuration(config_file='training_settings.ini')\n",
    "    sumo_cmd = set_sumo(True, config['sumocfg_file_name'], 3600)\n",
    "    path = set_train_path(config['models_path_name'])\n",
    "    model_path = get_model_path(config['models_path_name'], model_to_test)\n",
    "    opt = PPOOptions(entropy_coef = 0.33, T_horizon = int(2.5e3), eval_interval= 15, K_epochs=500, adv_normalization = True)\n",
    "    opt.dvc = torch.device(opt.dvc) # from str to torch.device\n",
    "    opt.state_dim = [3,48,46]\n",
    "    opt.action_dim = 8\n",
    "    opt.max_e_steps = 3600\n",
    "    n_cars_generated = 1000\n",
    "    green_duration = 7\n",
    "    yellow_duration = 6\n",
    "    total_episodes = 100\n",
    "    #opt.T_horizon = 2e4\n",
    "\n",
    "    agent = PPO_agent(**vars(opt)) #DRL_Model(num_layers, width_layers, batch_size, learning_rate, input_dim=num_states, output_dim=num_actions,tau=1,num_atoms=num_atoms)\n",
    "\n",
    "    trafficGen = TrafficGenerator(opt.max_e_steps, n_cars_generated)\n",
    "\n",
    "    visualization = Visualization(path, dpi=96)\n",
    "        \n",
    "    simulation = Simulation(agent,trafficGen,sumo_cmd,opt.max_e_steps,green_duration,yellow_duration,opt.state_dim,opt.action_dim, False, opt.dvc)\n",
    "\n",
    "    evaluation = Simulation(agent,trafficGen,sumo_cmd,opt.max_e_steps,green_duration,yellow_duration,opt.state_dim,opt.action_dim, True, opt.dvc)\n",
    "  \n",
    "    #Training = Training(Model,memory,training_epochs,update_epochs,batch_size)\n",
    "    \n",
    "    episode = 0\n",
    "    timestamp_start = datetime.datetime.now()\n",
    "    #traj_lenth = 0\n",
    "\n",
    "    while episode < total_episodes:\n",
    "        print('\\n----- Episode', str(episode+1), 'of', str(total_episodes))\n",
    "        print(agent.idx, agent.T_horizon)\n",
    "        #epsilon = 0.2 + (0.8 - 0.8*((episode+1) / total_episodes))  # set the epsilon for this episode according to epsilon-greedy policy\n",
    "        #epsilon = 0\n",
    "        simulation_time = simulation.run(episode)  # run the simulation\n",
    "        #print('Simulation time:', simulation_time, 's')\n",
    "        #training_time = Training.run()                      # train the model  \n",
    "        if (agent.idx) % opt.T_horizon == 0:\n",
    "            training_time = agent.train()\n",
    "            agent.idx = 0\n",
    "            print('Simulation time:', simulation_time, 's - Training time:', training_time, 's - Total:', round(simulation_time+training_time, 1), 's')\n",
    "        else:\n",
    "            print('Simulation time:', simulation_time, 's')\n",
    "        '''Record & log'''\n",
    "        if (episode+1) % opt.eval_interval == 0:\n",
    "            score, eval_time = evaluate_policy(evaluation, agent, turns=3) # evaluate the policy for 3 times, and get averaged result\n",
    "            print('Evaluation time:', eval_time, 's', 'Score:', score)\n",
    "            #if opt.write: writer.add_scalar('ep_r', score, global_step=total_steps)\n",
    "            #print('EnvName:',EnvName[0],'seed:',env_seed,'steps: {}k'.format(int(total_steps/1000)),'score:', score)        \n",
    "        episode += 1\n",
    "        \n",
    "    print(\"\\n----- Start time:\", timestamp_start)\n",
    "    print(\"----- End time:\", datetime.datetime.now())\n",
    "    print(\"----- Session info saved at:\", path)\n",
    "\n",
    "    Model.save_model(path)\n",
    "\n",
    "    copyfile(src='training_settings.ini', dst=os.path.join(path, 'training_settings.ini'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955569df-874f-4c63-96ad-cce707f482b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0250866e-07d0-446f-8c42-d5daa78a55a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL-env",
   "language": "python",
   "name": "rl-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
